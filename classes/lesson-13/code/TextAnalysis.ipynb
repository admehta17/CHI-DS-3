{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text analysis with Airline tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we'll import the pandas library and read in our csv that has a collection of tweets regarding different airlines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "airline_tweets = pd.read_csv(\"Tweets.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's inspect the data frame called \"airline_tweets\" and see the different variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "airline_tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to first create variables that will hold the variables of interest.\n",
    "\n",
    "In this case, we want to use the tweet's text to predict the sentiment of the tweet.\n",
    "\n",
    "That is, can we tell if the tweet is positive or negative automatically, just by the text.\n",
    "\n",
    "These tweets have already been tagged as positive, negative, and neurtal, so we can use that information to train the tweets.\n",
    "\n",
    "In the code below, we create a variable called \"sentiment\" that holds the sentiment of each positive or negative tweet.\n",
    "\n",
    "We then create a variable called \"text\" that holds the text of the tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Identify the variables of interest\n",
    "#Focus only on the positive and negative tweets: not neutral\n",
    "sentiment = airline_tweets.airline_sentiment[(airline_tweets.airline_sentiment != \"neutral\")]\n",
    "text = airline_tweets.text[(airline_tweets.airline_sentiment != \"neutral\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process the text\n",
    "\n",
    "Our next step is to process the text and get it in a correct format for analysis.\n",
    "\n",
    "The first thing we'll do is lowercase all of the text by using the .str.lower() function from the pandas data Series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lowercase the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    @virginamerica plus you've added commercials t...\n",
      "3    @virginamerica it's really aggressive to blast...\n",
      "4    @virginamerica and it's a really big bad thing...\n",
      "5    @virginamerica seriously would pay $30 a fligh...\n",
      "6    @virginamerica yes, nearly every time i fly vx...\n",
      "Name: text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "text = text.str.lower()\n",
    "print text.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll remove words that are probably not relevant to the tweet's sentiment, and might lead to overfitting.\n",
    "\n",
    "These words are called stopwords and they are conjunctions, articles, prepositions, and other function words in speech.\n",
    "\n",
    "We'll import a stopword list from sklearn.\n",
    "\n",
    "Then well make a function that filters out an word that is in the stopword list.\n",
    "\n",
    "Notice the content.split() function.\n",
    "\n",
    "That breaks our text apart into it's separate words\n",
    "\n",
    "After we've filter out the words, we'll join it back to a string and return the cleaned text\n",
    "\n",
    "Finally, we'll apply the function to every single row in our text data Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    @virginamerica plus you've added commercials e...\n",
      "3    @virginamerica it's really aggressive blast ob...\n",
      "4             @virginamerica it's really big bad thing\n",
      "5    @virginamerica seriously pay $30 flight seats ...\n",
      "6    @virginamerica yes, nearly time fly vx â€œear wo...\n",
      "Name: text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Make a list of the words you want remove# Remove stop words\n",
    "\n",
    "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n",
    "\n",
    "# Remove stop words\n",
    "def remove_stopwords(content):\n",
    "    cleaned = filter(lambda x: x not in ENGLISH_STOP_WORDS,content.split())\n",
    "    return ' '.join(cleaned)\n",
    "#Apply the function to every row\n",
    "text = text.apply(remove_stopwords)\n",
    "\n",
    "print text.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove Punctuation and Numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll remove punctuation from the tweets so that the punctuation isn't counted in the word.\n",
    "\n",
    "We'll apply our remove_punctuation function to each row of the data Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    virginamerica plus youve added commercials exp...\n",
      "3    virginamerica its really aggressive blast obno...\n",
      "4               virginamerica its really big bad thing\n",
      "5    virginamerica seriously pay  flight seats didn...\n",
      "6    virginamerica yes nearly time fly vx ear worm ...\n",
      "Name: text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "def remove_punctuation(content):\n",
    "    return filter(lambda x: x in string.ascii_letters+\" \",content)\n",
    "text = text.apply(remove_punctuation)\n",
    "print text.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reduce words to their word stems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finaly, we'll make all of our words word stems, so that, for example, see, sees, and seeing all become \"see\"\n",
    "\n",
    "We are going to use the Porter Stemmer algorithm from the porter stemmer library. For every word in our document's content, we'll apply the stem function to it, and then rejoin the words as a string.\n",
    "\n",
    "This stemming function is applied to every row in the data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1                                        virginamerica plu youv ad commerci experi tacki\n",
       "3    virginamerica it realli aggress blast obnoxi entertain guest face amp littl recours\n",
       "4                                                  virginamerica it realli big bad thing\n",
       "5            virginamerica serious pai flight seat didnt plai it realli bad thing fly va\n",
       "6                                 virginamerica ye nearli time fly vx ear worm wont awai\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Stemming\n",
    "import porterstemmer\n",
    "stemmer = porterstemmer.PorterStemmer()\n",
    "def stem_words(content):\n",
    "    stemmed_words = [stemmer.stem(word, 0,len(word)-1) for word in content.split()]\n",
    "    return \" \".join(stemmed_words)\n",
    "\n",
    "stemmed_text = text.apply(stem_words)\n",
    "stemmed_text.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The NLTK library also has a stemmer function. If you have nltk installed, you can run the function using the following code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'happi birthday'"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "def stem_words(content):\n",
    "    stemmed_words = [stemmer.stem(word) for word in content.split()]\n",
    "    return \" \".join(stemmed_words)\n",
    "stem_words(\"happy birthday\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a data Series with the text exactly as we want it, we can turn our text into a Document X Term using a CountVectorizer that we will fit (to learn the vocabulary) and transform on our text (to make a Document X Term matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vect = CountVectorizer()\n",
    "counts = count_vect.fit_transform(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Our Document X Term Matrix is stored in a sparse array to make it efficiently held in memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<11541x12152 sparse matrix of type '<type 'numpy.int64'>'\n",
       "\twith 112005 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the tfidf transformer to take our counts and make it into a Document X Term Matrix with tfidf values in the cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tf_transformer = TfidfTransformer(use_idf=True).fit(counts)\n",
    "tf_idf = tf_transformer.transform(counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although the matrix is in a sparse form, we can use the .todense() function to make it into a normal matrix with values for every cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "        ..., \n",
       "        [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0., ...,  0.,  0.,  0.]])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_idf.todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a matrix of our predictors (the tfidf values for each word-stem), we can enter them into a classifier.\n",
    "\n",
    "We'll use a Naive Bayes classifier, which is common for text classification, and predicts a category by assigning a probability to each word.\n",
    "\n",
    "We'll score the accuracy of what the classifier predicts and what the sentiment actually is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.87488085954423356"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf = MultinomialNB().fit(tf_idf, sentiment)\n",
    "clf.score(tf_idf,sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the confusion matrix to see how our true positive, true negative, false positive, and false negative rate is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[9172    6]\n",
      " [1438  925]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "sentiment_predictions = clf.predict(tf_idf)\n",
    "cm = confusion_matrix(sentiment,sentiment_predictions)\n",
    "print cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can visualize the confusion matrix as a heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1c5f05150>"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWMAAAD9CAYAAABp2RZmAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAADSFJREFUeJzt3V+MnNdZx/HvO2s7Rmh3kQKOQPwpveDJDbUUVaFOXTsU\nWyQWkPYmSBFSqIQj0ihClQAlRuYGGSO1slBANVLiENe5QGARhIhiVNWIrGsRXFThrDBPUlXlpiBU\nE3sTmibZermYsbpM3J3d7EzmnOPvJxo5e+Z43pNc/PbRc877TreysoIkabp6016AJMkwlqQiGMaS\nVADDWJIKYBhLUgEMY0kqwJZJfviHfmqv5+b0Ll95+a+nvQQVaNvcrd1mP2MjmXPxP/5x09cbp4mG\nsSS9n7quqHzdEMNYUjO6rt7Oa70rl6SGWBlLasZMxZWxYSypGT3DWJKmr+YNvHp/jUhSQ6yMJTWj\no97K2DCW1Ax7xpJUgJp7xoaxpGb0DGNJmr6u4jMJhrGkZtimkKQC2KaQpALUfLSt3gaLJDXEylhS\nMzxnLEkFmOkZxpI0dfaMJUmbYmUsqRn2jCWpAN70IUkF8KYPSSrAuDbwImILcBL4ALAMHAS+CzwD\nXAMWM/ORwdyDwEPAO8CRzHw+IrYDzwI7gCXgwcy8vNY1622wSNKQruvW/RrhADCTmR8F/gD4Q+AY\ncCgz9wK9iLgvIm4DHgV2AfcARyNiK/AwcDEz9wCngMOjLmgYS2pGr+vW/RrhFWBLRHTAPP2q947M\nXBi8/wKwH7gTOJeZy5m5BLwK7AR2A2dWzd036oK2KSQ1Y4znjN8Afhr4d+BW4JeBj616/3VgDpgF\nrg79vfmh8etz12RlLKkZva637tcInwHOZGbQr3S/AGxb9f4scIV+P3huaPy1wfjs0Ny1176e/0BJ\nusn8D9+rbK/Q7yJ8NSL2DsbuBRaAC8DuiNgWEfPA7cAicJ5+35nBn9fbG9+XbQpJzRjjOeM/Bp6O\niBeBrcBjwL8ATw026C4BpzNzJSKeAM4BHf0Nvrcj4jhwMiIWgLeAB0Zd0DCW1IyZMd2Bl5n/C/zq\nDd66+wZzTwAnhsbeBO7fyDUNY0nNqPmmD3vGklQAK2NJzfDZFJJUgJrbFIaxpGbU/HB5w1hSM6yM\nJakA9owlqQBWxpJUAHvGklSAmitjb/qQpAJYGUtqhht4klSAmtsUhrGkZqzjofHFqnflktQQK2NJ\nzejV26UwjCW1ww08SSqAG3iSVICaK2M38CSpAFbGkpoxri8knQbDWFIzau4Zr/vXSETU+ytH0k2h\n69b/Ks2alXFEfBA4BnwYWB4E8svAZzLzlfdhfZJ0UxjVpngKeDwzX7o+EBEfAf4c+OgkFyZJG9Vy\nm2L76iAGyMx/muB6JOk96zbwT2lGVcb/GhFPA2eAq8AscAC4OOmFSdJG1XzOeFQYfxr4BLAbmAOW\ngL8DnpvwuiRpw2YqfjjFmmGcmSv0g9fwlaQJ8pyxpGbUvIFnGEtqRokbc+tlGEtqhpWxJBWg4iw2\njCW1o+WjbZJUDdsUklSAirPYMJbUjporYx+LKUkFsDKW1AzPGUtSATxNIUkFaPZBQZJ0s4qIx4Bf\nAbYCnwdeBJ4BrgGLmfnIYN5B4CHgHeBIZj4fEduBZ4Ed9J92+WBmXl7rem7gSWpG13Xrfq0lIvYC\nuzLzLuBu4CfpfwXdoczcC/Qi4r6IuA14FNgF3AMcjYitwMPAxczcA5wCDo9au2EsqRm9bv2vEX4R\nWIyIvwH+lv5z3O/IzIXB+y8A+4E7gXOZuZyZS8CrwE76z4A/s2ruvlEXtE0hqRlj3MD7YfrV8C8B\nH6QfyKuL19fpf+HGLP1vQbruDWB+aPz63DUZxpKaMcbDFJeBS5m5DLwSEd8BfnzV+7PAFfr94Lmh\n8dcG47NDc9dkm0KS3u0c/R4wEfFjwA8CXxr0kgHuBRaAC8DuiNgWEfPA7cAicJ7+94Uy+HOBEayM\nJTVjphtPfTk4EfGxiPhnoKO/IfcN4KnBBt0l4HRmrkTEE/TDu6O/wfd2RBwHTkbEAvAW8MCoaxrG\nkpoxzns+MvOxGwzffYN5J4ATQ2NvAvdv5HqGsaRm+KAgSdKmWBlLaobPppCkAlScxYaxpHZYGUtS\nASp+aJthLKkdVsaSVICKs9gwltSOms8ZG8aSmlFzm8KbPiSpAFbGkppRcWFsGEtqR6/is22GsaRm\n1LyBZ89YkgpgZSypGRUXxoaxpHbUfLTNMJbUjIqz2DCW1A4rY0kqQMVZbBhLakfNR9sMY0nNqDiL\nDWNJ7ai5Z+xNH5JUACtjSc2ouDA2jCW1wwcFSVIB7BlLkjbFylhSMyoujA1jSe2ouU1hGEtqRsVZ\nPNkwPnv66CQ/XpV687++Oe0lqEDb5m7d9Gd4O7QkFaDiLDaMJbXDnrEkFaDiLDaMJbWj8w48SZq+\nmitj78CTpAJYGUtqhht4klQAn9omSQWouDA2jCXp+4mIHcBXgH3Ad4FngGvAYmY+MphzEHgIeAc4\nkpnPR8R24FlgB7AEPJiZl9e6lht4ktrRdet/jRARW4A/A749GDoGHMrMvUAvIu6LiNuAR4FdwD3A\n0YjYCjwMXMzMPcAp4PCo6xnGkprRdd26X+vwOeA48E2gA+7IzIXBey8A+4E7gXOZuZyZS8CrwE5g\nN3Bm1dx9oy5mGEtqxrgK44j4deC/M/OL9IMY/n9evg7MAbPA1VXjbwDzQ+PX567JnrGkZozxDrxP\nAdciYj/9SvcLwI+sen8WuEK/Hzw3NP7aYHx2aO6aDGNJzRjXaYpBXxiAiDgL/Cbw2YjYk5kvAvcC\nZ4ELwJGI2Ab8AHA7sAicBw7Q3/w7ACwwgmEsqRkTvunjt4EnBxt0l4DTmbkSEU8A5+i3Mw5l5tsR\ncRw4GRELwFvAA6M+3DCW1IxJZHFmfnzVj3ff4P0TwImhsTeB+zdyHcNYUjNqvh3a0xSSVAArY0nN\nqLgwNowltaObqTeNDWNJzbBnLEnaFCtjSc2ouDA2jCW1o+Y2hWEsqRkVZ7FhLKkhFaexYSypGWN8\natv7zjCW1IyKC2PDWFI73MCTpAJUnMXe9CFJJbAyltSOiktjw1hSMzxNIUkFqDmM7RlLUgGsjCU1\no+KWsWEsqR01tykMY0nN8KYPSSpBvVnsBp4klcDKWFIzer1660vDWFI76s1iw1hSO2rewKv494gk\ntcPKWFIzaq6MDWNJ7ag3iw1jSe3wDjxJKoFtCkmavoqz2DCW1A438CSpBPaMJWn6aq6MvelDkgpg\nZSypGc0ebYuIfwBuGRrugJXMvGtiq5Kk96DZMAYeA54EPgksT345krQJFfeM1wzjzHwpIk4BH8rM\n596nNUnSezKuDbyI2AI8DXwA2AYcAf4NeAa4Bixm5iODuQeBh4B3gCOZ+XxEbAeeBXYAS8CDmXl5\nrWuO3MDLzM8axJJuMr8GfCsz9wD3AH8KHAMOZeZeoBcR90XEbcCjwK7BvKMRsRV4GLg4+PungMOj\nLuhpCknt6DbwWttf8r0AnaHfpr0jMxcGYy8A+4E7gXOZuZyZS8CrwE5gN3Bm1dx9oy7oaQpJzRjX\nBl5mfhsgImaBvwJ+D/jcqimvA3PALHB11fgbwPzQ+PW5a7IyltSMrtdb92uUiPgJ4CxwMjP/gn6v\n+LpZ4Ar9fvDc0Phrg/HZoblrMowlacigF/z3wO9m5snB8FcjYs/g3+8FFoALwO6I2BYR88DtwCJw\nHjgwmHtgMHdNtikktWN854wfB34IOBwRvw+sAL8F/Mlgg+4ScDozVyLiCeAc/U70ocx8OyKOAycj\nYgF4C3hg1AW7lZWVcS3+Xb514fzkPlzV2jo/O3qSbjrzP/Ozm07S/zz7pXVnzo9+/BeKOpRsZSyp\nHUXF68YYxpKa4VPbJEmbYmUsqRndTL31pWEsqR0VtykMY0nNsGcsSdoUK2NJ7Wj44fKSVI2a2xSG\nsaR2GMaSNH0tfweeJNXDyliSps+esSSVwDCWpOmruWfsTR+SVAArY0ntsE0hSdO3ni8aLZVhLKkd\n9owlSZthZSypGV1Xb31pGEtqhxt4kjR93oEnSSWoeAPPMJbUDCtjSSqBYSxJBfA0hSRNnw8KkiRt\nipWxpHbYM5ak6et6M9NewntmGEtqhj1jSdKmWBlLaoc9Y0maPu/Ak6QSeNOHJBWg4g08w1hSM2xT\nSFIJbFNI0vRZGUtSCSqujOtduSQ1xMpYUjNqvh3aMJbUDnvGkjR9NT+1rVtZWZn2GiTppucGniQV\nwDCWpAIYxpJUAMNYkgpgGEtSAQxjSSqA54wnLCI64PPATuA7wG9k5tenuyqVICJ+DvijzPz5aa9F\n02dlPHmfAG7JzLuAx4FjU16PChARvwM8Cdwy7bWoDIbx5O0GzgBk5kvAh6e7HBXia8Anp70IlcMw\nnrw54Oqqn5cjwv/vN7nMfA5YnvY6VA5DYfKWgNlVP/cy89q0FiOpTIbx5H0ZOAAQER8BXp7uclSY\neh8zprHyNMXkPQfsj4gvD37+1DQXo+L4pC4BPrVNkopgm0KSCmAYS1IBDGNJKoBhLEkFMIwlqQCG\nsSQVwDCWpAIYxpJUgP8DkfNKarznSRkAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1c5f05710>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn\n",
    "% matplotlib inline\n",
    "seaborn.heatmap(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we wanted to adjust the rate of false positives and false negatives, we can use the class-prior parameter in the naive bayes classifier to make predictions of a certain class more likely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9214972706\n",
      "[[8546  632]\n",
      " [ 274 2089]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x114e62450>"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWMAAAD9CAYAAABp2RZmAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAADpRJREFUeJzt3W/I3eV9x/H3707V6HYnMEakc0zoA7+yB0ZU0uoy06Jm\nbfan9sEmyEYsm8GsZOgDhwbEIWRtt6mQSTNQ16bKYFimLUqsoyv2vitJY3BLA+6rRbZnMslibsOm\nMfPswTmB0yjn3DHn5FzXlfcrHLzP7/7l97vig8/58r2u63e6Xq+HJGm25mY9AEmSYSxJRTCMJakA\nhrEkFcAwlqQCGMaSVIBPTPPiV1y6wXVz+pC9e7896yGoQBd98tLuTK9xOplz8D9fPOP7TdJUw1iS\nzqauKypfT4thLKkZXVdv57XekUtSQ6yMJTVjRcWVsWEsqRlzhrEkzV7NE3j1foxIUkOsjCU1o6Pe\nytgwltQMe8aSVICae8aGsaRmzBnGkjR7XcVrEgxjSc2wTSFJBbBNIUkFqHlpW70NFklqiJWxpGa4\nzliSCrBizjCWpJmzZyxJOiNWxpKaYc9Ykgrgpg9JKoCbPiSpAJOawIuIzcBtQA+4EFgLXAc8C7w2\nOG1XZj4VEbcDW4D3gR2Z+VxErASeBNYAS8DmzDw86p6GsaRmTKpNkZm7gd0AEfEI8DhwNfBgZj58\n8ryIuBjYBlwFXAQsRsQLwFbgYGY+EBG3APcBd466Z73dbkk6xVzXLfu1HBFxDfDrmfkY/TD+7Yh4\nMSIejYhfBNYBi5l5IjOXgNfpV9HrgecHl9kD3Dh27Kf/z5WkMnWn8WeZ7gX+YvDzPuDuzNwAvAHc\nD6wCjg6dfwxYDcwPHX9ncN5IhrGkZsx1c8t+jRMRq4HLMvNHg0PPZOYrJ38GrqQfuMNBOw8cod8n\nnh869vbYsS/rXyhJ557rgR8Mvf/+oG0BcANwANgPrI+I8wfhfTlwCHgJ2DQ4dxOwMO5mTuBJasaE\n1xkH/XbESXcAj0TEceBNYEtmHouIncAi0AHbM/N4ROwCdkfEAvAecOvYsfd6vUkO/udccemG6V1c\n1dq799uzHoIKdNEnLz3jJL3lmj9edub848uPF7Uo2cpYUjNq3vRhz1iSCmBlLKkZPptCkgpQc5vC\nMJbUjJofLm8YS2qGlbEkFcCesSQVwMpYkgpgz1iSClBzZeymD0kqgJWxpGY4gSdJBai5TWEYS2rG\nch4aX6p6Ry5JDbEyltSMuXq7FIaxpHY4gSdJBXACT5IKUHNl7ASeJBXAylhSM1ZUvLTNMJbUjJp7\nxsv+GImIej9yJJ0Tum75r9KMrIwj4lPAQ8A1wIlBIP8UuCszXzsL45Okc8K4NsVjwL2Zue/kgYj4\nDPBN4DemOTBJOl0ttylWDgcxQGbuneJ4JOlj607jT2nGVcb/FhF/DzwPHAXmgU3AwWkPTJJOV83r\njMeF8Z8CNwPrgVXAEvAs8PSUxyVJp21FxQ+nGBnGmdmjH7yGryRNkeuMJTWj5gk8w1hSM0qcmFsu\nw1hSM6yMJakAk8ziiLgH+D3gPOAbwI+AbwEfAIcy8yuD824HtgDvAzsy87mIWAk8Cayhv/Bhc2Ye\nHnU/tzhLakbXdct+jRIRG4BrM/M64LPAr9Hfjbw9MzcAcxHxxYi4GNgGXAt8HvhqRJwHbAUOZub1\nwBPAfePGbhhLasZc1y37NcZvAYci4hnge/SX9F6VmQuD3+8BbgLWAYuZeSIzl4DXgbX0lwM/P3Tu\njeNuaJtCUjMm2Kb4ZfrV8O8An6IfyMPF6zv0917M098Qd9IxYPUpx0+eO5JhLKkZE5zAOwy8mpkn\ngNci4l3gV4d+Pw+8Tb8fvOqU40cGx+dPOXck2xSS9GGL9HvARMSvAL8A/GDQSwb4ArAA7AfWR8T5\nEbEauBw4BLxE/9ERDP67wBhWxpKaMal1xoMVEb8ZET8BOvoTcv8BPDaYoHsV+E5m9iJiJ/3w7uhP\n8B2PiF3A7ohYAN4Dbh13T8NYUjMm+aCgzLznIw5/9iPOexx4/JRj/wv8wenczzCW1IyaHxRkz1iS\nCmBlLKkZLT/PWJKqUXGXwjCW1A4rY0kqQMVZ7ASeJJXAylhSM1Z09daXhrGkZtTcpjCMJTWj5m/6\nqLeml6SGWBlLaoZL2ySpABVnsWEsqR1WxpJUALdDS1IBrIwlqQAVZ7FhLKkdNa8zNowlNaPmNoWb\nPiSpAFbGkppRcWFsGEtqx1zFa9sMY0nNqHkCz56xJBXAylhSMyoujA1jSe2oeWmbYSypGRVnsWEs\nqR1WxpJUgIqz2DCW1I6al7YZxpKaUXEWG8aS2lFzz9hNH5JUACtjSc2YdGEcEWuAl4EbgYuAZ4HX\nBr/elZlPRcTtwBbgfWBHZj4XESuBJ4E1wBKwOTMPj7qXYSypGZN8UFBEfAL4O+B/BoeuBh7MzIeH\nzrkY2AZcRT+sFyPiBWArcDAzH4iIW4D7gDtH3c8wltSMCfeM/wbYBdw7eH81cFlE3Ey/Or4LWAcs\nZuYJYCkiXgfWAuuBrw/+3h76YTySPWNJOkVE3Ab8V2b+M9ANXvuAuzNzA/AGcD+wCjg69FePAauB\n+aHj7wzOG8kwltSMrlv+a4wvAzdFxA+BK4HdwJ7MfGXw+2cGx4/y80E7Dxyh3yeeHzr29rgb2qaQ\n1IxJtSkG1S8AEfEvwB3A9yJiW2buB24ADgD7gR0RcT5wIXA5cAh4CdhEf/JvE7Aw7p6GsaRmTHmZ\n8R3AIxFxHHgT2JKZxyJiJ7BIv5WxPTOPR8QuYHdELADvAbeOu3jX6/WmNvLjS4end3FV6629B2Y9\nBBXoko0bzzhKX/jzXcvOnI1/tbWoHSJWxpKaUfEGPMNYUjtq3g5tGEtqRsVZbBhLakc3wR14Z5th\nLKkZNVfGbvqQpAJYGUtqhhN4klSAST617WwzjCU1o+LC2J6xJJXAylhSOyoujQ1jSc1wAk+SClBx\nFhvGktrhDjxJKoCVsSQVwJ6xJBWg4iw2jCW1o+bK2E0fklQAK2NJzai4MDaMJbWjW1FvGhvGkpph\nz1iSdEasjCU1o+LC2DCW1I6a2xSGsaRmVJzFhrGkhlScxoaxpGb41DZJKkDFhbFhLKkdTuBJUgEq\nzmI3fUhSCayMJbVjQqVxRMwBjwIBfADcAbwHfGvw/lBmfmVw7u3AFuB9YEdmPhcRK4EngTXAErA5\nMw+PuqeVsaRmdHPdsl9j/C7Qy8z1wH3AXwIPAdszcwMwFxFfjIiLgW3AtcDnga9GxHnAVuBgZl4P\nPDG4xkiGsaRmTCqMM/O79KtdgEuBI8BVmbkwOLYHuAlYByxm5onMXAJeB9YC64Hnh869cdzYDWNJ\n+giZ+UFEfAvYCfwDMJzg7wCrgHng6NDxY8DqU46fPHckw1hSM7pu+a/lyMzbgMuAx4ALh341D7xN\nvx+86pTjRwbH5085dyTDWFIzJtWmiIg/jIh7Bm/fBf4PeDkiNgyOfQFYAPYD6yPi/IhYDVwOHAJe\nAjYNzt00OHckV1NIasYEN338E/DNiHiRfk7+GfDvwGODCbpXge9kZi8idgKL9NsY2zPzeETsAnZH\nxAL9VRi3jh17r9eb1OA/5PjS4eldXNV6a++BWQ9BBbpk48YzTtLc/dSyMyc2/35RW0RsU0hSAWxT\nSGrG3Fy99aVhLKkd9WaxYSypHTU/ta3izxFJaoeVsaRm1FwZG8aS2lFvFhvGktrhd+BJUglsU0jS\n7FWcxYaxpHY4gSdJJbBnLEmzV3Nl7KYPSSqAlbGkZjS7tC0ifghccMrhjv63pl43tVFJ0sfQbBgD\n9wCPAl8CTkx/OJJ0BiruGY8M48zcFxFPAFdk5tNnaUyS9LHUPIE3tmecmX99NgYiSecyJ/AktaPe\nwtgwltSOlifwJKkaXcXfgVfvyCWpIVbGktphm0KSZq/ppW2SVI16s9gwltSOmitjJ/AkqQBWxpKa\n0a2ot740jCW1o+I2hWEsqRn2jCVJZ8TKWFI73PQhSbM36TZFRHwa+Fpmfi4irgSeBV4b/HpXZj4V\nEbcDW4D3gR2Z+VxErASeBNYAS8DmzDw86l6GsaR2TDCMI+Ju4I+AY4NDVwMPZubDQ+dcDGwDrgIu\nAhYj4gVgK3AwMx+IiFuA+4A7R93PMJbUjAk/QvNn9L9y7onB+6uByyLiZvrV8V3AOmAxM08ASxHx\nOrAWWA98ffD39tAP45GcwJPUjq5b/muMwVfNDX/35z7g7szcALwB3A+sAo4OnXMMWA3MDx1/Z3De\nSIaxpGZ0Xbfs18fwTGa+cvJn4Er6gTsctPPAEfp94vmhY2+Pu7hhLKkdE6yMP8L3I+Kawc83AAeA\n/cD6iDg/IlYDlwOHgJeATYNzNwEL4y5uz1hSM6b8tUtbgb+NiOPAm8CWzDwWETuBRfrPjNuemccj\nYhewOyIWgPeAW8ddvOv1elMb+fGlw9O7uKr11t4Dsx6CCnTJxo1nnKT//a8/WXbm/NKV64palGxl\nLKkdFW+HNowlNaPmLyQ1jCW1o+Lt0PV+jEhSQ6yMJTWj6+qtLw1jSe1wAk+SZq/mh8sbxpLaUfEE\nnmEsqRlWxpJUAsNYkgrgagpJmr0pPyhoqur9GJGkhlgZS2qHPWNJmr1ubsWsh/CxGcaSmmHPWJJ0\nRqyMJbXDnrEkzZ478CSpBG76kKQCVDyBZxhLaoZtCkkqgW0KSZo9K2NJKkHFlXG9I5ekhlgZS2pG\nzduhDWNJ7bBnLEmzV/NT27perzfrMUjSOc8JPEkqgGEsSQUwjCWpAIaxJBXAMJakAhjGklQA1xlP\nWUR0wDeAtcC7wJ9k5huzHZVKEBGfBr6WmZ+b9Vg0e1bG03czcEFmXgfcCzw04/GoABFxN/AocMGs\nx6IyGMbTtx54HiAz9wHXzHY4KsTPgC/NehAqh2E8fauAo0PvT0SE/9/PcZn5NHBi1uNQOQyF6VsC\n5ofez2XmB7MajKQyGcbT92NgE0BEfAb46WyHo8LU+5gxTZSrKabvaeCmiPjx4P2XZzkYFccndQnw\nqW2SVATbFJJUAMNYkgpgGEtSAQxjSSqAYSxJBTCMJakAhrEkFcAwlqQC/D/3UsDVm2dRIAAAAABJ\nRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x158bfdb10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "clf = MultinomialNB(class_prior=[.4,.6]).fit(tf_idf, sentiment)\n",
    "print clf.score(tf_idf,sentiment)\n",
    "sentiment_predictions = clf.predict(tf_idf)\n",
    "cm = confusion_matrix(sentiment,sentiment_predictions)\n",
    "print cm\n",
    "seaborn.heatmap(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try with a Support Vector Machine\n",
    "\n",
    "Another common classifier is a support vector machine.\n",
    "\n",
    "These classifiers tend to work well when there are many predictors such as our current matrix which has counts for all of the words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.999566761979\n",
      "[[9177    1]\n",
      " [   4 2359]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1c685d950>"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWMAAAD9CAYAAABp2RZmAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nUgEMY0kqgGEsSQX4P2c+TgGLeq2MAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x20a494950>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "clf = SVC(C=10,gamma=1)\n",
    "clf.fit(tf_idf, sentiment)\n",
    "print clf.score(tf_idf,sentiment)\n",
    "sentiment_predictions = clf.predict(tf_idf)\n",
    "cm = confusion_matrix(sentiment,sentiment_predictions)\n",
    "print cm\n",
    "seaborn.heatmap(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use our predictions list to isolate just the texts are predicted to be positive and see what they say"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "889                                     united great fares\n",
       "8677      jetblue yes yes does great trip down thanks lift\n",
       "6502        southwestair timeand just time flight thursday\n",
       "12595    americanair gerrielliott will chance get thank...\n",
       "5463               southwestair fortunemagazine great news\n",
       "2896                        thank united prompt assistance\n",
       "6597     southwestair great flight amp crew las vegasch...\n",
       "6975     jetblue excited hear international travel long...\n",
       "8633     jetblue handled xjareds question like social m...\n",
       "9145     usairways great crew flight  phx yvr tonight f...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[(sentiment_predictions == \"positive\")].sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Likewise, we can do the same for tweets predicted to be negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4661     southwestair can doesnt fair bags need fit sea...\n",
       "7689     jetblue doing horrible job jfkyou need rethink...\n",
       "10740    usairways again flight week delayed  mins thin...\n",
       "12524    americanair pity machine replied story unusual...\n",
       "3261     united direct phone number status match desk r...\n",
       "10818    usairways ive tried calling  times past  days ...\n",
       "11649    usairways did wont help cant believe wouldnt f...\n",
       "13424    americanair talked anyone guys prepared situat...\n",
       "7314     jetblue concerned delay notified looked board ...\n",
       "9725     usairways delayed  hours longbeachairport tryi...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[(sentiment_predictions == \"negative\")].sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to identify the underlying reasons or topics for negative complaints.\n",
    "\n",
    "We'll make a variable that just holds the negative reviews and make a Document X Term tfidf weighted matrix from those"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "negative_reviews = text[(sentiment == 'negative')]\n",
    "count_vect = CountVectorizer()\n",
    "counts = count_vect.fit_transform(negative_reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll fit a Latent Dirilecht Allocation model to the matrix, and say that we expect there to about 5 reasons (an arbritrary number basedon an educated guess).\n",
    "\n",
    "We'll also say to have a maximum of 10 iterations when fitting the model to give the model more time to try to find the best topic distributions. The downside of a larger iteration number is that it takes more time to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LatentDirichletAllocation(batch_size=128, doc_topic_prior=None,\n",
       "             evaluate_every=-1, learning_decay=0.7,\n",
       "             learning_method='online', learning_offset=10.0,\n",
       "             max_doc_update_iter=100, max_iter=50, mean_change_tol=0.001,\n",
       "             n_jobs=1, n_topics=5, perp_tol=0.1, random_state=None,\n",
       "             topic_word_prior=None, total_samples=1000000.0, verbose=0)"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_topics = 5\n",
    "lda = LatentDirichletAllocation(n_topics=n_topics,max_iter=10)\n",
    "lda.fit(counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can discover what the topics represent by printing out the top 10 words associated with each.\n",
    "\n",
    "Try to think about what each topic means based on the words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "united jetblue service customer americanair southwestair plane usairways delay dont\n",
      "americanair flight united usairways im time late delayed hours just\n",
      "usairways hold thanks worst help minutes weather change phone thats\n",
      "jetblue issue refund pay united whats answer speak understand better\n",
      "flight cancelled southwestair usairways flightled airport help united need flights\n"
     ]
    }
   ],
   "source": [
    "n_top_words = 10\n",
    "import operator\n",
    "vocabulary = count_vect.get_feature_names()\n",
    "for i in range(n_topics):\n",
    "    best_words_indexes = lda.components_[i].argsort()[:-n_top_words - 1:-1]\n",
    "    best_words = \" \".join([vocabulary[i] for i in best_words_indexes])\n",
    "    print best_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see how each document scored on each topic using the transform function on our lda model class.\n",
    "\n",
    "We'll call each topic \"topic1\" \"topic2\" etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_topics = 5\n",
    "topic_assignments = lda.transform(counts)\n",
    "topic_names = [\"topic%s\" % i for i in range(1,n_topics+1)]\n",
    "topic_assignments = pd.DataFrame(topic_assignments,range(counts.shape[0]), topic_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Larger values mean that the document is more about that topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic1</th>\n",
       "      <th>topic2</th>\n",
       "      <th>topic3</th>\n",
       "      <th>topic4</th>\n",
       "      <th>topic5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.202320</td>\n",
       "      <td>0.200276</td>\n",
       "      <td>7.191488</td>\n",
       "      <td>0.201795</td>\n",
       "      <td>0.204120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.202925</td>\n",
       "      <td>1.203036</td>\n",
       "      <td>11.185380</td>\n",
       "      <td>0.205008</td>\n",
       "      <td>0.203651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6.190344</td>\n",
       "      <td>0.200101</td>\n",
       "      <td>0.204148</td>\n",
       "      <td>0.202359</td>\n",
       "      <td>0.203048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6.163506</td>\n",
       "      <td>0.200042</td>\n",
       "      <td>0.203825</td>\n",
       "      <td>0.205135</td>\n",
       "      <td>7.227492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.202124</td>\n",
       "      <td>0.200113</td>\n",
       "      <td>0.202143</td>\n",
       "      <td>8.024803</td>\n",
       "      <td>2.370818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.201840</td>\n",
       "      <td>0.200254</td>\n",
       "      <td>0.200622</td>\n",
       "      <td>4.194497</td>\n",
       "      <td>0.202788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.202890</td>\n",
       "      <td>0.200074</td>\n",
       "      <td>2.078666</td>\n",
       "      <td>1.544722</td>\n",
       "      <td>4.973647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7.197838</td>\n",
       "      <td>0.200133</td>\n",
       "      <td>0.200276</td>\n",
       "      <td>0.200609</td>\n",
       "      <td>0.201143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.200828</td>\n",
       "      <td>0.200038</td>\n",
       "      <td>0.201670</td>\n",
       "      <td>0.205320</td>\n",
       "      <td>13.192143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>12.176550</td>\n",
       "      <td>0.200081</td>\n",
       "      <td>0.205894</td>\n",
       "      <td>0.215192</td>\n",
       "      <td>0.202283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.204131</td>\n",
       "      <td>0.200423</td>\n",
       "      <td>0.201320</td>\n",
       "      <td>2.187676</td>\n",
       "      <td>0.206449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.200999</td>\n",
       "      <td>0.200127</td>\n",
       "      <td>0.200334</td>\n",
       "      <td>0.200768</td>\n",
       "      <td>4.197772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2.618536</td>\n",
       "      <td>0.200041</td>\n",
       "      <td>0.201009</td>\n",
       "      <td>0.208900</td>\n",
       "      <td>11.771514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1.666584</td>\n",
       "      <td>0.200121</td>\n",
       "      <td>11.724264</td>\n",
       "      <td>0.206787</td>\n",
       "      <td>0.202244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.204191</td>\n",
       "      <td>0.200232</td>\n",
       "      <td>0.200714</td>\n",
       "      <td>0.202161</td>\n",
       "      <td>2.192703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.201564</td>\n",
       "      <td>0.200133</td>\n",
       "      <td>1.506182</td>\n",
       "      <td>7.890325</td>\n",
       "      <td>0.201796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.200983</td>\n",
       "      <td>0.200115</td>\n",
       "      <td>0.200333</td>\n",
       "      <td>7.195964</td>\n",
       "      <td>0.202605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>4.195183</td>\n",
       "      <td>0.200212</td>\n",
       "      <td>0.200491</td>\n",
       "      <td>0.201984</td>\n",
       "      <td>0.202130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.203488</td>\n",
       "      <td>0.209802</td>\n",
       "      <td>0.201495</td>\n",
       "      <td>7.183536</td>\n",
       "      <td>0.201679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>11.947973</td>\n",
       "      <td>0.200048</td>\n",
       "      <td>0.201393</td>\n",
       "      <td>2.448687</td>\n",
       "      <td>0.201900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2.254127</td>\n",
       "      <td>0.200053</td>\n",
       "      <td>0.208037</td>\n",
       "      <td>0.205224</td>\n",
       "      <td>8.132560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>11.492335</td>\n",
       "      <td>0.200070</td>\n",
       "      <td>0.200129</td>\n",
       "      <td>3.906041</td>\n",
       "      <td>0.201425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.208554</td>\n",
       "      <td>0.200071</td>\n",
       "      <td>0.200124</td>\n",
       "      <td>0.201947</td>\n",
       "      <td>11.189304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.201394</td>\n",
       "      <td>0.200064</td>\n",
       "      <td>1.300398</td>\n",
       "      <td>17.097147</td>\n",
       "      <td>0.200997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.201610</td>\n",
       "      <td>0.200039</td>\n",
       "      <td>0.200654</td>\n",
       "      <td>3.629882</td>\n",
       "      <td>10.767816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.200865</td>\n",
       "      <td>0.202895</td>\n",
       "      <td>0.200454</td>\n",
       "      <td>11.194887</td>\n",
       "      <td>0.200899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.200738</td>\n",
       "      <td>0.200118</td>\n",
       "      <td>0.200967</td>\n",
       "      <td>9.975965</td>\n",
       "      <td>1.422211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.202559</td>\n",
       "      <td>0.200088</td>\n",
       "      <td>3.833061</td>\n",
       "      <td>0.202758</td>\n",
       "      <td>6.561534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.207511</td>\n",
       "      <td>0.200238</td>\n",
       "      <td>8.180519</td>\n",
       "      <td>0.208452</td>\n",
       "      <td>0.203279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>6.196789</td>\n",
       "      <td>0.200122</td>\n",
       "      <td>0.200313</td>\n",
       "      <td>0.200711</td>\n",
       "      <td>0.202066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11511</th>\n",
       "      <td>3.838439</td>\n",
       "      <td>0.200010</td>\n",
       "      <td>1.281483</td>\n",
       "      <td>3.428138</td>\n",
       "      <td>1.251930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11512</th>\n",
       "      <td>1.556187</td>\n",
       "      <td>0.200031</td>\n",
       "      <td>2.682142</td>\n",
       "      <td>5.337685</td>\n",
       "      <td>5.223955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11513</th>\n",
       "      <td>2.930913</td>\n",
       "      <td>2.191783</td>\n",
       "      <td>1.201450</td>\n",
       "      <td>4.989171</td>\n",
       "      <td>1.686684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11514</th>\n",
       "      <td>1.303894</td>\n",
       "      <td>1.199407</td>\n",
       "      <td>6.576583</td>\n",
       "      <td>3.718724</td>\n",
       "      <td>0.201393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11515</th>\n",
       "      <td>4.212770</td>\n",
       "      <td>0.200013</td>\n",
       "      <td>3.800750</td>\n",
       "      <td>3.481939</td>\n",
       "      <td>2.304529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11516</th>\n",
       "      <td>0.200671</td>\n",
       "      <td>0.200007</td>\n",
       "      <td>1.619185</td>\n",
       "      <td>10.197927</td>\n",
       "      <td>1.782211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11517</th>\n",
       "      <td>3.041340</td>\n",
       "      <td>1.201566</td>\n",
       "      <td>1.285135</td>\n",
       "      <td>11.270839</td>\n",
       "      <td>0.201120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11518</th>\n",
       "      <td>2.421200</td>\n",
       "      <td>1.188050</td>\n",
       "      <td>2.595013</td>\n",
       "      <td>6.549759</td>\n",
       "      <td>1.245978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11519</th>\n",
       "      <td>0.200549</td>\n",
       "      <td>0.200033</td>\n",
       "      <td>2.429949</td>\n",
       "      <td>9.774970</td>\n",
       "      <td>1.394500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11520</th>\n",
       "      <td>3.168926</td>\n",
       "      <td>0.200005</td>\n",
       "      <td>3.408967</td>\n",
       "      <td>8.832679</td>\n",
       "      <td>2.389423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11521</th>\n",
       "      <td>2.005864</td>\n",
       "      <td>1.199488</td>\n",
       "      <td>1.394593</td>\n",
       "      <td>1.200052</td>\n",
       "      <td>0.200003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11522</th>\n",
       "      <td>0.200003</td>\n",
       "      <td>1.198359</td>\n",
       "      <td>5.016912</td>\n",
       "      <td>0.201572</td>\n",
       "      <td>2.383155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11523</th>\n",
       "      <td>0.204616</td>\n",
       "      <td>0.200009</td>\n",
       "      <td>5.633715</td>\n",
       "      <td>2.721891</td>\n",
       "      <td>1.239770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11524</th>\n",
       "      <td>7.859361</td>\n",
       "      <td>0.200348</td>\n",
       "      <td>3.079407</td>\n",
       "      <td>0.203444</td>\n",
       "      <td>1.657439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11525</th>\n",
       "      <td>3.460469</td>\n",
       "      <td>0.200003</td>\n",
       "      <td>2.365306</td>\n",
       "      <td>6.771189</td>\n",
       "      <td>0.203033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11526</th>\n",
       "      <td>2.354858</td>\n",
       "      <td>0.200007</td>\n",
       "      <td>1.564785</td>\n",
       "      <td>4.680055</td>\n",
       "      <td>0.200295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11527</th>\n",
       "      <td>2.419477</td>\n",
       "      <td>0.200006</td>\n",
       "      <td>9.978166</td>\n",
       "      <td>0.202066</td>\n",
       "      <td>0.200284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11528</th>\n",
       "      <td>0.200278</td>\n",
       "      <td>0.200003</td>\n",
       "      <td>1.908547</td>\n",
       "      <td>1.476038</td>\n",
       "      <td>4.215135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11529</th>\n",
       "      <td>3.931779</td>\n",
       "      <td>0.200005</td>\n",
       "      <td>2.679110</td>\n",
       "      <td>0.205091</td>\n",
       "      <td>3.984014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11530</th>\n",
       "      <td>6.621483</td>\n",
       "      <td>0.200005</td>\n",
       "      <td>1.248947</td>\n",
       "      <td>2.729378</td>\n",
       "      <td>1.200187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11531</th>\n",
       "      <td>1.445435</td>\n",
       "      <td>0.200006</td>\n",
       "      <td>2.557678</td>\n",
       "      <td>9.571890</td>\n",
       "      <td>1.224991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11532</th>\n",
       "      <td>0.200128</td>\n",
       "      <td>7.199398</td>\n",
       "      <td>4.199529</td>\n",
       "      <td>0.200081</td>\n",
       "      <td>0.200864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11533</th>\n",
       "      <td>0.200862</td>\n",
       "      <td>1.200449</td>\n",
       "      <td>1.295942</td>\n",
       "      <td>5.102500</td>\n",
       "      <td>1.200247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11534</th>\n",
       "      <td>1.199860</td>\n",
       "      <td>0.200002</td>\n",
       "      <td>1.200104</td>\n",
       "      <td>1.200032</td>\n",
       "      <td>0.200001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11535</th>\n",
       "      <td>4.450122</td>\n",
       "      <td>0.200005</td>\n",
       "      <td>3.235597</td>\n",
       "      <td>2.910663</td>\n",
       "      <td>0.203613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11536</th>\n",
       "      <td>0.201216</td>\n",
       "      <td>0.200002</td>\n",
       "      <td>1.234181</td>\n",
       "      <td>13.147693</td>\n",
       "      <td>1.216907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11537</th>\n",
       "      <td>2.726685</td>\n",
       "      <td>0.200004</td>\n",
       "      <td>1.673146</td>\n",
       "      <td>0.200163</td>\n",
       "      <td>0.200002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11538</th>\n",
       "      <td>0.204632</td>\n",
       "      <td>0.200001</td>\n",
       "      <td>4.777781</td>\n",
       "      <td>1.617579</td>\n",
       "      <td>0.200007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11539</th>\n",
       "      <td>10.101399</td>\n",
       "      <td>0.200130</td>\n",
       "      <td>2.959951</td>\n",
       "      <td>0.204729</td>\n",
       "      <td>2.533791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11540</th>\n",
       "      <td>2.604898</td>\n",
       "      <td>2.199673</td>\n",
       "      <td>1.386935</td>\n",
       "      <td>3.492134</td>\n",
       "      <td>1.316360</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11541 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          topic1    topic2     topic3     topic4     topic5\n",
       "0       0.202320  0.200276   7.191488   0.201795   0.204120\n",
       "1       0.202925  1.203036  11.185380   0.205008   0.203651\n",
       "2       6.190344  0.200101   0.204148   0.202359   0.203048\n",
       "3       6.163506  0.200042   0.203825   0.205135   7.227492\n",
       "4       0.202124  0.200113   0.202143   8.024803   2.370818\n",
       "5       0.201840  0.200254   0.200622   4.194497   0.202788\n",
       "6       0.202890  0.200074   2.078666   1.544722   4.973647\n",
       "7       7.197838  0.200133   0.200276   0.200609   0.201143\n",
       "8       0.200828  0.200038   0.201670   0.205320  13.192143\n",
       "9      12.176550  0.200081   0.205894   0.215192   0.202283\n",
       "10      0.204131  0.200423   0.201320   2.187676   0.206449\n",
       "11      0.200999  0.200127   0.200334   0.200768   4.197772\n",
       "12      2.618536  0.200041   0.201009   0.208900  11.771514\n",
       "13      1.666584  0.200121  11.724264   0.206787   0.202244\n",
       "14      0.204191  0.200232   0.200714   0.202161   2.192703\n",
       "15      0.201564  0.200133   1.506182   7.890325   0.201796\n",
       "16      0.200983  0.200115   0.200333   7.195964   0.202605\n",
       "17      4.195183  0.200212   0.200491   0.201984   0.202130\n",
       "18      0.203488  0.209802   0.201495   7.183536   0.201679\n",
       "19     11.947973  0.200048   0.201393   2.448687   0.201900\n",
       "20      2.254127  0.200053   0.208037   0.205224   8.132560\n",
       "21     11.492335  0.200070   0.200129   3.906041   0.201425\n",
       "22      0.208554  0.200071   0.200124   0.201947  11.189304\n",
       "23      0.201394  0.200064   1.300398  17.097147   0.200997\n",
       "24      0.201610  0.200039   0.200654   3.629882  10.767816\n",
       "25      0.200865  0.202895   0.200454  11.194887   0.200899\n",
       "26      0.200738  0.200118   0.200967   9.975965   1.422211\n",
       "27      0.202559  0.200088   3.833061   0.202758   6.561534\n",
       "28      0.207511  0.200238   8.180519   0.208452   0.203279\n",
       "29      6.196789  0.200122   0.200313   0.200711   0.202066\n",
       "...          ...       ...        ...        ...        ...\n",
       "11511   3.838439  0.200010   1.281483   3.428138   1.251930\n",
       "11512   1.556187  0.200031   2.682142   5.337685   5.223955\n",
       "11513   2.930913  2.191783   1.201450   4.989171   1.686684\n",
       "11514   1.303894  1.199407   6.576583   3.718724   0.201393\n",
       "11515   4.212770  0.200013   3.800750   3.481939   2.304529\n",
       "11516   0.200671  0.200007   1.619185  10.197927   1.782211\n",
       "11517   3.041340  1.201566   1.285135  11.270839   0.201120\n",
       "11518   2.421200  1.188050   2.595013   6.549759   1.245978\n",
       "11519   0.200549  0.200033   2.429949   9.774970   1.394500\n",
       "11520   3.168926  0.200005   3.408967   8.832679   2.389423\n",
       "11521   2.005864  1.199488   1.394593   1.200052   0.200003\n",
       "11522   0.200003  1.198359   5.016912   0.201572   2.383155\n",
       "11523   0.204616  0.200009   5.633715   2.721891   1.239770\n",
       "11524   7.859361  0.200348   3.079407   0.203444   1.657439\n",
       "11525   3.460469  0.200003   2.365306   6.771189   0.203033\n",
       "11526   2.354858  0.200007   1.564785   4.680055   0.200295\n",
       "11527   2.419477  0.200006   9.978166   0.202066   0.200284\n",
       "11528   0.200278  0.200003   1.908547   1.476038   4.215135\n",
       "11529   3.931779  0.200005   2.679110   0.205091   3.984014\n",
       "11530   6.621483  0.200005   1.248947   2.729378   1.200187\n",
       "11531   1.445435  0.200006   2.557678   9.571890   1.224991\n",
       "11532   0.200128  7.199398   4.199529   0.200081   0.200864\n",
       "11533   0.200862  1.200449   1.295942   5.102500   1.200247\n",
       "11534   1.199860  0.200002   1.200104   1.200032   0.200001\n",
       "11535   4.450122  0.200005   3.235597   2.910663   0.203613\n",
       "11536   0.201216  0.200002   1.234181  13.147693   1.216907\n",
       "11537   2.726685  0.200004   1.673146   0.200163   0.200002\n",
       "11538   0.204632  0.200001   4.777781   1.617579   0.200007\n",
       "11539  10.101399  0.200130   2.959951   0.204729   2.533791\n",
       "11540   2.604898  2.199673   1.386935   3.492134   1.316360\n",
       "\n",
       "[11541 rows x 5 columns]"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_assignments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see what are the tweets that scored most highly on a certain topic (e.g., topic1).\n",
    "\n",
    "The code below sorts the topic assignments by their score on topic 1, then gets the top 10 indexes.\n",
    "\n",
    "We'll make the pandas column width larger to be able to read the tweets.\n",
    "\n",
    "Finally, we'll take the indexes of the top 10 tweets and index our main text column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1565                  united ua heading gate bc improper load ur packing gills care load bags expecting comp\n",
      "259      virginamerica requested window seat confirmed window got stuck middle seat good way treat silver...\n",
      "10817    usairways know like gate x nationalairpor  nothini like nothin bout yo ghettofab gate usairways ...\n",
      "24       virginamerica guys messed seating reserved seating friends guys gave seat away   want free internet\n",
      "633                   united way simple apology goes long way its hollow one its obvious dont care wellbeing\n",
      "13252    americanair united just deplaned ohare gate k dont send customer service rep discuss situation g...\n",
      "997      united guy really customer service cluecould spent effort clearing bins rollerboards instead art...\n",
      "9162     usairways instead fair treatment got nasty letter lady senior management illness forfeits miles ...\n",
      "1543     united truly drunk uncle boarding dont believe gate agent overhead full dont reserve aisle httpt...\n",
      "2414                  united its doubtful hell fly united traveling lot sales manager covering north america\n",
      "Name: text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "topic1_top10 = topic_assignments.sort_values(by='topic1',ascending = False).index[:10]\n",
    "pd.set_option('max_colwidth', 100)\n",
    "print negative_reviews.iloc[topic1_top10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.set_option('max_colwidth', 800)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does that topic seem to match with your personal guess of what the topic was? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see what the different coded topics were and see if they match up to the topic assignments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([nan, 'Bad Flight', \"Can't Tell\", 'Late Flight',\n",
       "       'Customer Service Issue', 'Flight Booking Problems', 'Lost Luggage',\n",
       "       'Flight Attendant Complaints', 'Cancelled Flight',\n",
       "       'Damaged Luggage', 'longlines'], dtype=object)"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "airline_tweets['negativereason'].unique()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
