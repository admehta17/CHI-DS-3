{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First we start by importing the libraries we'll use\n",
    "We want to import the print_dunction library so we can use the print() command\n",
    "\n",
    "We want to import from sklearn the TfidfVectorizer and CountVectorizer (found in the \"text\" sublibrary of the \"feature_extraction\" sublibrary)\n",
    "\n",
    "We want to import the LatentDirilechtAllocation function from the decomposition sublibrary of sklearn. This function is the primary function for topic modelling\n",
    "\n",
    "We want to import the textclean library to format our input text and make it more uniform (lower case, no punctation, no extra spaces, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import textclean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We want to define a function that prints the top N words associated with a topic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_top_words(model, feature_names, n_top_words):\n",
    "\tfor topic_idx, topic in enumerate(model.components_):\n",
    "\t\ttopic_idx += 1\n",
    "\t\tprint(\"Topic #%d:\" % topic_idx)\n",
    "\t\tprint(\" \".join([feature_names[i]\n",
    "\t\t\t\t\t\tfor i in topic.argsort()[:-n_top_words - 1:-1]]))\n",
    "\tprint()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We now want to load in the data.\n",
    "\n",
    "Our data is the first chapter of the book, \"Doing Data Science\"\n",
    "\n",
    "Each paragraph is on a new line\n",
    "\n",
    "We open the file in read-binary mode (\"rb\") and then read every line into a list called \"dataset\", where each line a new element in the list\n",
    "\n",
    "We can print the first paragraph using the 0 index of the list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Loading dataset...\")\n",
    "dataset = open(\"..\\dataset\\doingdatascience1.txt\",\"rb\").readlines()\n",
    "print(\"Dataset loaded\")\n",
    "print(dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  We now want to process the text make everything lowercase, remove punctuation, remove extra spaces, remove non-printable characters\n",
    "\n",
    "The cleaned data is saved in a variable called data_samples\n",
    "\n",
    "We can print the first line again to see how it changed\n",
    "\n",
    "Because the documents are in a list, we can see how many documents we have using the len function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_samples = [textclean.process(line) for line in dataset]\n",
    "print(data_samples[0])\n",
    "n_samples = len(dataset)\n",
    "print(\"Number of documents: \"+str(n_samples))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### We want to define the parameters for the topic model\n",
    "\n",
    "We want to only use the 500 most common words in our text for calculating the topic model\n",
    "\n",
    "We want the topic model to have 3 topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_features = 500\n",
    "n_topics = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We now start the analysis by creating a matrix of documents (rows) and how often each of the top 300 words occurs\n",
    "\n",
    "We specify other features we want:\n",
    "\n",
    "We want a maximum frequency for a word, where it cannot occur is more than 95% of the documents\n",
    "\n",
    "We want a minimum frequency for words. A word has to occur in at least 2 documents to be considered in the final feature list\n",
    "\n",
    "We want stopwords (\"the\", \"is\", \"an\", \"of\", etc.) removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Use tf (raw term count) features for LDA.\n",
    "print(\"Extracting tf features for LDA...\")\n",
    "tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2, max_features=n_features, stop_words='english')\n",
    "tf = tf_vectorizer.fit_transform(data_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We are fitting a topic model on our matrix of documents and words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"Fitting LDA models with tf features, n_samples=%d and n_features=%d...\"\n",
    "      % (n_samples, n_features))\n",
    "lda = LatentDirichletAllocation(n_topics=n_topics, max_iter=5, learning_method='online', learning_offset=50., random_state=0)\n",
    "t0 = time()\n",
    "lda.fit(tf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We want to see the top 10 words associated with each topic\n",
    "\n",
    "We use our \"print_top_words\" function that we created at the top"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_top_words = 10\n",
    "print(\"\\nTopics in LDA model:\")\n",
    "tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "print_top_words(lda, tf_feature_names, n_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
