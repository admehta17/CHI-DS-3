{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Downloading CSVs directly from the web\n",
    "\n",
    "Imagine you are doing market research on crime in an area.\n",
    "\n",
    "Therefore you need to access historical crime data for as many years as possible to determine the frequency of crime and also examine seasonal trends.\n",
    "\n",
    "The city of Chicago provides a large selection of crime data indicating the time, location, and type:\n",
    "Visit this url to see an example (Crime data for 2015): https://data.cityofchicago.org/Public-Safety/Crimes-2015/vwwp-7yr9\n",
    "\n",
    "We can download the data as a csv file by clicking the blue \"export\" button at the top-right of the page, and then clicking \"CSV\" link that appears (Where it says, \"Download As\")\n",
    "\n",
    "Notice that the link to download the csv is: https://data.cityofchicago.org/api/views/vwwp-7yr9/rows.csv\n",
    "\n",
    "Also, notice that the link to download a csv datafile from the city of Chicago has the following format: \"https://data.cityofchicago.org/api/views/\" and then the last part of the page's URL (vwwp-7yr9) and then \"rows.csv\"\n",
    "\n",
    "If we knew what the url was for each csv file, we can easily have Python automatically download all of the crime data \n",
    "\n",
    "Our goal for this activity is going to be to have a script that finds out what the links are for all of those files, and then downloads the csv files for is.\n",
    "\n",
    "You can use the search box on the page, \"https://data.cityofchicago.org/\"\n",
    "\n",
    "This is the url for the first page of data that has the crime data\n",
    "https://www.metrochicagodata.org/browse?q=crime&page=1\n",
    "\n",
    "We ideally want to write a script that goes through EVERY page of results, and downloads the link name, and then accesses the data.\n",
    "\n",
    "The following script is an example of how we would download the results page from page 1, and get the HTML tree to find the links in the page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import lxml.html as ET\n",
    "\n",
    "#This is the url for the first page of data that has the crime data\n",
    "url=\"https://www.metrochicagodata.org/browse?q=crime&page=1\"\n",
    "\n",
    "#send the request\n",
    "content= requests.get(url)\n",
    "\n",
    "#save the page source code to a string called content_string\n",
    "content_string = content.text.encode(\"utf-8\")\n",
    "\n",
    "#pass the page source to our html parse\n",
    "doc = ET.document_fromstring(content_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extracting the links\n",
    "\n",
    "Look at the page source on the data page: https://www.metrochicagodata.org/browse?q=crime&page=1\n",
    "\n",
    "Our goal is to know what the links are for each dataset so we can use them to download the data.\n",
    "\n",
    "What tag does each link have around it, and what properties do those tags have?\n",
    "\n",
    "You should notice that each data link is surrounded by an <a> tag\n",
    "\n",
    "All of the links to datasets have a class='browse2-result-name-link'\n",
    "\n",
    "Therefore, we want to access all of the <a> tags with the class equal to browse2-result-name-link\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "links = doc.xpath(\"//a[@class='browse2-result-name-link']\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retreiving the links\n",
    "\n",
    "For every link that matched the criteria we asked for, we can get it's url, which is stored in the 'href' attribute within its <a> tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#make an empty empty to hold the data_urls and their titles (as the key)\n",
    "data_urls = {}\n",
    "\n",
    "#for every link in out list of links\n",
    "for link in links:\n",
    "    \n",
    "    #check to make sure the data is relevant by seeing if the link text has \"Crimes -\" in it, which is the format the data tend to follow\n",
    "    if \"Crimes - \" in link.text:\n",
    "        #if the link text is relevant to our interest, add it to our url list\n",
    "       data_urls[link.text] = link.attrib['href'] \n",
    "\n",
    "print data_urls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finish the process - Make a complete script to download all of the Crime data\n",
    "\n",
    "Make a complete script that downloads the crime data and save it to your computer.\n",
    "\n",
    "The comments help guide the code you should write. The code is also indented to help guide your code writing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#### Write the full code here\n",
    "\n",
    "#make an empty dictionary to hold the urls and titles of the data file pages\n",
    "\n",
    "\n",
    "#for every page number from 1-4\n",
    "\n",
    "    #have mechanize go to the results page for the page number\n",
    "    #remember, the results page url for page 1 was: \"https://www.metrochicagodata.org/browse?q=crime&page=1\"\n",
    "    \n",
    "    \n",
    "    #get the page's source code and save it to a variable that is a string\n",
    "    \n",
    "    \n",
    "    #parse the source code into an html tree and save as a variable called doc\n",
    "    \n",
    "    \n",
    "    #get the list of links from the results page that we save\n",
    "    \n",
    "    \n",
    "    #for every link in the list of links\n",
    "        \n",
    "        #add the link to the dictionary, using the link text as the key\n",
    "        \n",
    "    \n",
    "#For every key,url in the dictionary.items():\n",
    "    \n",
    "    #get the ending of the url (the part right after the /) and save it as a variable called linkending\n",
    "    #Hint: use.split(\"/\")[-1]\n",
    "    #.split() allows you to break up a string on a character of your choice and the result is a list, where the elements are the contents that were separated by the character\n",
    "    #\"hello/how/are/you\".split(\"/\") would produce [\"hello\",\"how\",\"are\",\"you\"]\n",
    "    \n",
    "    \n",
    "    #download the csv \n",
    "    #The code to download files from urls in Python is:\n",
    "   \n",
    "    \n",
    "    import urllib #import the library to grab the file and save it to your computer\n",
    "    \n",
    "    \n",
    "    url= \"https://domain.com/subdirectory/filename.csv\" #edit this to be what you need\n",
    "    \n",
    "    #remember, the url for city of Chicago data uses the following structure:\n",
    "    #\"https://data.cityofchicago.org/api/views/linkending/rows.csv\"\n",
    "    #linkending should be the information you parsed when you used split\n",
    "    #to join strings together, you can use the + sign\n",
    "    #hello\" + \"!\" results in \"hello!\"\n",
    "    \n",
    "\n",
    "    filename=\"whatever_you_want_to_name_the_file.csv\" #edit this to be the text link\n",
    "    urllib.urlretrieve(url, filename) #saves the file found at the url to your computer using the filename you provided\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
