{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Guided Practice: Logit Function and Odds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Logit function\n",
    "\n",
    "The logit function is the function/transformation we apply to our outcome variable in a logistic regression.\n",
    "\n",
    "Our predictors, when summed, give us the log odds of our outcome occurring - the logit\n",
    "\n",
    "We can then convert the logit our model predicts into a probability using the sigmoid function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def logit_func(odds):\n",
    "    # uses a float (odds) and returns back the log odds (logit)\n",
    "    return np.log(odds)\n",
    "\n",
    "def sigmoid_func(logit):\n",
    "    # uses a float (logit) and returns back the probability\n",
    "    return 1. / (1 + np.exp(-logit))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Odds can be any positive number from 0 to infinity, and are typically expressed as a fraction or ratio.\n",
    "\n",
    "Odds = 1 : 1 means that either outcome (success or failure) is equally likely.\n",
    "\n",
    "Odds = 2 : 1 means that the first outcome (success) is twice as likely as the second outcome (failure)\n",
    "\n",
    "Odds = 1 : 2 means that the second outcome (failure) is twice as likely as the first outcome)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "odds_set = [\n",
    "    1.0 / 1.0,\n",
    "    2.0 / 1.0,\n",
    "    1.0 / 2.0,\n",
    "    6.0 / 6.0,\n",
    "    99999999999999.0 / 1.0,\n",
    "     1.0 / 99999999999999.0\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice below how the sigmoiud function can take the log odds and transform them to a probability equivalent to what the odds indicate\n",
    "\n",
    "When the odds are 1:1, and each outcome is equally likely, we get the odds of sucess as .50 or 50%\n",
    "\n",
    "Also, no matter how large our odds are, the probability returned can never be above 1.0 or below 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for odds in odds_set:\n",
    "    print sigmoid_func(logit_func(odds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic regression\n",
    "\n",
    "Logistic regression uses the same form a linear regression, where our outcome is equal to:\n",
    "\n",
    "$$y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + .. + \\beta_n X_n \\\\ $$\n",
    " \n",
    "The outcome however is the the log odds of y.\n",
    "\n",
    "$$\\log{\\frac{p}{1-p}} = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + .. + \\beta_n X_n \\\\ $$\n",
    "\n",
    "The coefficients represent the increase in the log odds of our outcome occurring for 1 unit increase in X."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load in the college admission's data\n",
    "\n",
    "We're going to read in the college admission data that contian information on a student's gre score, their gpa, and their school rank\n",
    "\n",
    "Again:\n",
    "\n",
    "'admit' is a binary variable. It indicates whether or not a candidate was admitted admit =1) our not (admit= 0)\n",
    "\n",
    "'gre' is GRE score\n",
    "\n",
    "'gpa' stands for Grade Point Average\n",
    "\n",
    "'rank' is the rank of an applicant's undergraduate alma mater, with 1 being the highest and 4 as the lowest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('collegeadmissions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rank is currently from 1 to 4, though we can treat it as categorical because the difference between each rank may not be the exact same.\n",
    "\n",
    "The following code makes rank into a dummy variables, and keeps ranks 1 2 and 3 as variables, which are then joined to the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = df.join(pd.get_dummies(df['rank'],prefix=\"rank\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at how our dataframe now has the 4 additional dummy variables corresponding to the four ranks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's enter in all of our predictors into the model that tries to predict admission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictors=['gre', 'gpa', \"rank_1\", \"rank_2\", \"rank_3\"]\n",
    "lm = LogisticRegression()\n",
    "lm.fit(df[predictors], df['admit'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can view the coefficients for each predictor. These coefficients represent the increase in log odds of admission for 1 unit increase in the variable, holding all of the other variables constant\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "zip(predictors,lm.coef_[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how as a person's GRE score increases by 1 unit, the log odds of being admitted increases by .001639\n",
    "\n",
    "Because it might be easier to think in terms of odds and not log odds, we can exponentiate the coefficients to interpret them as increasing the odds of being admitted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# You can easily convert these into odds using numpy.exp()\n",
    "import numpy as np\n",
    "zip(predictors,np.exp(lm.coef_)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the odds of being admitted multiply by 1.0016 for a 1 unit increase in GRE score.\n",
    "\n",
    "Also, the above makes it more clear that a schools rank as it approaches 4 decreases the odds of getting admitted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can evaluate the accuracy of the model by using the score function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "feature_set = df[predictors]\n",
    "print lm.score(feature_set, df['admit'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy of the model with all features (removing one rank) is ~70%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing and tuning a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets's examine the titanic dataset which contains information on whether a passenger died or survived when the ship sank. The dataset also contains information on variables associated with each passenger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "titanic = pd.read_csv('titanic.csv')\n",
    "titanic.set_index('PassengerId', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can create variables that match predictions of interest.\n",
    "\n",
    "For example, we might believe that a person's class on the ship predicts whether that person suvived.\n",
    "\n",
    "We might also predict that being male decreased a person's chance of surviving\n",
    "\n",
    "Age may also have an effect, where perhaps younger passengers were more likely to survive, or people with parents and siblings aboard the ship"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "titanic = titanic.join(pd.get_dummies(titanic.Pclass))\n",
    "titanic['is_male'] = titanic.Sex.apply(lambda x: 1 if x == 'male' else 0)\n",
    "titanic['Age'] = titanic.groupby([\"Sex\", 'Pclass']).Age.transform(lambda x: x.fillna(x.mean()))\n",
    "titanic['had_parents'] = titanic.Parch.apply(lambda x: 1 if x > 0 else 0)\n",
    "titanic['had_siblings'] = titanic.SibSp.apply(lambda x: 1 if x > 0 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "titanic.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before creating the model, we can visualize differences in survival by plotting two histograms of the variable: one for each outcome.\n",
    "\n",
    "Notice the slight mean difference between histograms, indicating that sex may indeed to survival probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "titanic.groupby('Survived').Age.hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We wil fit the model using a cross-validated grid search of the parameters. For logistic regression, we can have a regularization weight, C, which prevents overfitting, and a class weight to bias our predictions towards one particular class (if think the baserate probability of each outcome isn't 50/50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import grid_search, cross_validation\n",
    "\n",
    "feature_set = titanic[['is_male', 1, 2, 'Fare', 'Age', 'had_parents', 'had_siblings']]\n",
    "\n",
    "gs = grid_search.GridSearchCV(\n",
    "    estimator=LogisticRegression(),\n",
    "    param_grid={'C': [10**-i for i in range(-5, 5)], 'class_weight': [None, 'balanced']},\n",
    "    cv=cross_validation.KFold(n=len(titanic), n_folds=10),\n",
    "    scoring='roc_auc'\n",
    ")\n",
    "\n",
    "\n",
    "gs.fit(feature_set, titanic['Survived'])\n",
    "gs.grid_scores_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll take the best performing estimator from the grid search and save it to a variable called \"lm\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lm = gs.best_estimator_\n",
    "print lm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below the ROC curve is based on various thresholds: it shows with a false positive rate (x-axis) ~0, it also expects a true positive rate (y-axis) ~0 (the same, ish, for the top right hand of the figure).\n",
    "\n",
    "The second chart, which does not play with thesholds, shows the one true TPR and FPR point, joined to 0,0 and 1,1.\n",
    "\n",
    "The first chart will be more effective as you compare models and determine where the decision line should exist for the data. The second simplifies the first in case this idea of thresholds is confusing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "actuals = lm.predict(feature_set) \n",
    "probas = lm.predict_proba(feature_set)\n",
    "print len(probas)\n",
    "plt.plot(roc_curve(titanic['Survived'], probas[:,1])[0], roc_curve(titanic['Survived'], probas[:,1])[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, you can use the `roc_auc_score` function to calculate the area under these curves (AUC)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "roc_auc_score(titanic['Survived'], lm.predict(feature_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cancer Prediction Problem\n",
    "\n",
    "The following data set are taken from actual breast cancer screening images.\n",
    "\n",
    "They contain whether the breast mass was malignant (cancer) or begnin.\n",
    "\n",
    "Features are computed from a digitized image of a fine needle aspirate (FNA) of a breast mass.  A few of the images can be found at: http://www.cs.wisc.edu/~street/images/\n",
    "\n",
    "Your goal is to try to predict the status of the mass, with special consideration to the desired classification metric.\n",
    "\n",
    "Here is a full data dictionary for the data:\n",
    "\n",
    "1) ID number\n",
    "2) Diagnosis (M = malignant, B = benign)\n",
    "3-32)\n",
    "\n",
    "Ten real-valued features are computed for each cell nucleus:\n",
    "\n",
    "a) radius (mean of distances from center to points on the perimeter)\n",
    "\n",
    "b) texture (standard deviation of gray-scale values)\n",
    "\n",
    "c) perimeter\n",
    "\n",
    "d) area\n",
    "\n",
    "e) smoothness (local variation in radius lengths)\n",
    "\n",
    "f) compactness (perimeter^2 / area - 1.0)\n",
    "\n",
    "g) concavity (severity of concave portions of the contour)\n",
    "\n",
    "h) concave points (number of concave portions of the contour)\n",
    "\n",
    "i) symmetry\n",
    "\n",
    "j) fractal dimension (\"coastline approximation\" - 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Goals **\n",
    "\n",
    "1. Spend a few minutes determining which data would be most important to use in the prediction problem. You may need to create new features based on the data available. Consider using a feature selection aide in sklearn. But a worst case scenario; identify one or two strong features that would be useful to include in the model.\n",
    "2. Spend 1-2 minutes considering which _metric_ makes the most sense to optimize. Accuracy? FPR or TPR? AUC?\n",
    "3. Build a tuned Logistic model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cancer = pd.read_csv('cancer.csv')\n",
    "cancer.head()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
